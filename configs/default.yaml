model:
  name: "Qwen/Qwen3-1.7B"
  torch_dtype: "bfloat16"
  device_map: "auto"

chunker:
  chunk_size: 1024
  overlap: 128
  max_chunks: 64

latent_extractor:
  extraction_layers: [7, 14, 21, 27]
  pooling: "last_token"

page_compressor:
  d_page: 512

page_aggregator:
  num_soft_tokens: 16
  num_heads: 8
  num_agg_layers: 1

training:
  learning_rate: 3.0e-4
  weight_decay: 0.05
  batch_size: 4
  epochs: 10
  warmup_steps: 200
  gradient_clip: 1.0
  patience: 8
  min_delta: 0.001
  lambda_recon: 0.0
  use_question_conditioning: false

baseline:
  chunk_size: 1024
  max_buffer_tokens: 4096

dataset:
  train_samples: 2000
  val_samples: 300
  test_samples: 500
  min_doc_tokens: 8192
  max_doc_tokens: 32768
  test_max_doc_tokens: 65536
  source: "mixed"

evaluation:
  max_new_tokens: 128

seeds:
  torch: 42
  numpy: 42
  random: 42
